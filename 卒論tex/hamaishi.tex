\documentclass{jarticle}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{ascmac}
\usepackage{url}



\title{強化学習を利用したAIディレクターの制作}
\author{濱石海地}
\date{2021年11月2日}

\setlength{\textwidth}{16truecm}
\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\begin{document}

%\maketitle
%\clearpage
\begin{titlepage}
    \begin{center}

        
        \vspace*{50truept}

        {\Huge 卒業論文}

        \vspace{50truept}

        {\Huge 強化学習を利用したAIディレクターの制作} 

        \vspace{130truept}

        {\huge 甲南大学　知能情報学部}

        \vspace{10truept}

        {\huge 知能情報学科}

        \vspace{10truept}

        {\huge 11771083}

        \vspace{10truept}

        {\huge 濱石 海地}

        \vspace{80truept}

        {\huge 2021年11月2日} 
     
       \vspace{10truept}     
	  {\huge 指導教授　田中雅博} 
    \end{center}
\end{titlepage}

\tableofcontents
\clearpage

\section{はじめに}
ゲームにおいて難易度は、それ一つでゲームの面白さを左右する重要な要素であり、ゲーム開発において最も工数をかけるべき箇所のひとつである。
一般的にはゲームの進行と共にプレイヤーの腕が向上するため、ゲームが進行するほど難易度も上がるよう調整される場合がほとんどである。
しかし、プレイヤーの腕や、面白く感じる難易度は人それぞれであり、またその調整は試行錯誤を繰り返すなどして調整する必要があり、工数の多さはゲーム開発者にとって大きな負担となる。
それを解決するべく、当研究では「AIディレクター」に着目した。AIディレクターとは、Valve Software社開発のシューティングゲーム「Left 4 Dead」に使用されている難易度調整システムの呼称であり、それはプレイヤー達の体力や所持品、敵との戦い方からプレイヤーの状態を推定し、敵の配置などを調整している。呼称や手法は異なるものの、プレイヤーの状況によって難易度を秘密裏に調整するシステムは「バイオハザード4」など他のゲームにも見られる。
これら「AIディレクター」は、多くはルールベースによって作られていると考えられる。当研究はこれを機械学習を用いて作成することで、ユーザーそれぞれに最適な難易度を提供すると共に、開発者の負担を軽減することの実現可能性を探るものである。

%アピールする新規性/有用性:
%従来のAIディレクターはルールベースである。
%各々のプレイヤーに対して適切な難易度を提供し、娯楽性を最大にする。
%ゲーム制作者が難易度調整の工数を削減できる。


\section{研究の概要}
この研究のために作成されたオリジナルのゲームにおいて、登場する敵やアイテムの出現を強化学習AIに制御させる。

\subsection{ゲームの仕様}
Pythonのコマンドライン上で動作する、簡素なRPG。

\subsubsection{ゲーム全体の流れ}
以下の流れを繰り返すことで進行する。
これを所定の回数(10回あるいは20回)繰り返した後、プレイヤーが生存しているならゲームクリアとする。
\begin{enumerate}
\item 2通りの行先が提示されるので、プレイヤーはそれを選ぶ。行先に何があるかは表示されている。
強化学習AIはここで何が提示されるのかを選ぶ。
\item 選んだ行先に敵がいるなら、それと戦う。アイテムがあるなら、それを獲得する。
\end{enumerate}
図\ref{playersFlow}(\pageref{playersFlow}ページ)を参照のこと。

\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.3]{playersFlow.eps}
\end{center}
\caption{ゲームを遊ぶ流れのイメージ図}
\label{playersFlow}
\end{figure}

\subsubsection{戦闘の流れ}
戦闘ではプレイヤーと敵が、どちらかの生命力が0以下になるまで、
「プレイヤー、敵、プレイヤー、敵…」の順番で交互に行動する。
戦闘における行動は表\ref{tableBattleAction}(\pageref{tableBattleAction}ページ)の通り。
\begin{table}[hpbt]
\caption{戦闘における行動}
\label{tableBattleAction}
\begin{center}
\begin{tabular}{cl}
\\ \hline
攻撃 & 相手に自身の攻撃力と同値のダメージを与える(生命力を減らす)。 \\ \hline
防御 & 相手から受けるダメージを、自身の防御力の値だけダメージを減少させる。 \\ \hline
薬草を使用 & 薬草所持数を1消費し、自身の生命力を全回復。 \\ \hline
爆弾を使用 & 爆弾所持数を1消費し、攻撃力2倍の攻撃を行う。 \\ \hline
\end{tabular}
\end{center}
\end{table}
プレイヤーの行動はコンソールからの手動入力、あるいはプログラムによる自動入力で選ばれる。
敵の行動は、確率に応じてランダムに選ばれる。

\subsubsection{プレイヤーおよび敵のステータス(能力値)}
\begin{table}[hpbt]
\caption{プレイヤーおよび敵のステータス(能力値)}
\label{tableBattleStatus}
\begin{center}
\begin{tabular}{cl}
\\ \hline
生命力 & これが0になったら死亡。アイテム等により回復する。 \\ \hline
最大生命力 & 生命力はこの値を超えて回復しない。アイテム拾得により増加。 \\ \hline
攻撃力 & 戦闘において敵に与えるダメージ量に影響。\\
 & 戦闘に勝利するか、特定のアイテム拾得により増加。 \\ \hline
防御力 & 戦闘において防御したとき、この値だけ受けるダメージ量が減る。\\
 & 戦闘に勝利するか、特定のアイテム拾得により増加。  \\ \hline
薬草所持数 & プレイヤー専用。戦闘中に消費して生命力を回復できる。\\
 & アイテム拾得により増加。  \\ \hline
爆弾所持数 & プレイヤー専用。戦闘中に消費して攻撃力2倍の攻撃ができる。\\
 & アイテム拾得により増加。 \\ \hline
\end{tabular}
\end{center}
\end{table}
プレイヤーは6つの、敵は4つの値をステータスとして持ち、それらは戦闘中の行動に影響する。表\ref{tableBattleStatus}(\pageref{tableBattleStatus}ページ)を参照。

\subsection{プラグラム及びソースコードの構造}
ゲームを遊ぶプレイヤーと、ゲームに登場する敵・アイテムを操作するディレクターは、該当部分のクラスを差し替えることで、
他のゲーム部分を編集することなく作り変えることが可能である。
図\ref{UML_class}(\pageref{UML_class}ページ)に示すクラス図を参照。
\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.2]{UML_class.eps}
\end{center}
\caption{当研究に使用したプログラムのクラス図}
\label{UML_class}
\end{figure}

\newpage

\section{学習の方法と結果}
以上を踏まえ、ディレクターを学習させることでゲームの難易度を最適なものとすることを試みる。
当研究における最適な難易度とは、ルールベースで自動的にプレイするするプレイヤーが、ゲームを7割進行させた所でちょうどゲームオーバーとなるような難易度としている。

\subsubsection{各手法に共通する大まかな流れ}
ディレクターに適切な難易度を学習させるためには、難易度を測る指標必要がある。
当研究ではディレクターの調整した難易度を測るために、ゲームを複数回プレイし、それらのプレイでどこまで進めることができたか(以下、最終到達階層)の平均を求め、それを難易度の指標としている。
\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.3]{learnFlow.eps}
\end{center}
\caption{学習全体の流れ。図中の数値は研究初期の一例}
\label{learnFlow}
\end{figure}

ディレクタークラスには2つのグローバル関数があり、一つがディレクションを返す関数、もう一つが学習を行わせる関数である。\\
ディレクションを返す関数には、引数としてプレイヤーの各種能力、出現する敵やアイテムのリストを渡す。返り値として出現させるべき敵およびアイテムのリストが渡される。
内部ではプレイヤーの能力値を入力値に、出現する敵やアイテムの価値を出力とするニューラルネットワークの、前向き計算が行われる。それと同時にディレクションをクラス内に保存している。\\
学習を行わせる関数には、引数としてゲームの平均到達階層を渡す。返り値は無い。
引数を基にディレクタークラス側で保存されていたディレクションのスコアを計算し、スコアを教師データとしてニューラルネットワークの誤差修正を行う。
\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.3]{directorMechanics.eps}
\end{center}
\caption{ディレクターの仕組み}
\label{directorMechanics}
\end{figure}

\subsection{一般的なε-グリーディ法を使用した方法}
まず最初に一般的な強化学習の手法に見られるような、
出力値のうち最も高い値2つを選択肢として提示する方法をとった。

\subsubsection{この手法の手順} \label{logic_greedy}
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、AIディレクターの出力(以下、ディレクションとする)と、最終到達階層を記録し、それらを基にスコアを算出する。
\item 1.と2.を10回行い、データを溜める(最大で2000データ、10プレイ*最大20階*10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ゲーム中におけるディレクターの構造}
入力値7個、出力値17個の、Chainerを利用したニューラルネットワーク。
入力値は、プレイヤーのステータス(\pageref{tableBattleStatus}ページ,表\ref{tableBattleStatus}を参照)の値6つに、現在のプレイヤー位置の値を合わせた実数値7個。
出力値は、出現する全ての敵11種類と、出現する全アイテム6種類、それぞれの価値を決める実数値17個。それらのステータスについては表\ref{tableGameItems}(\pageref{tableGameItems}ページ)を参照。
出力値のうち、最も大きい値2つが示している敵またはアイテムがゲームに出現する。

\begin{table}[hpbt]
\caption{ゲーム中に出現する敵およびアイテムのリスト}
\label{tableGameItems}
\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{4}{|c|}{敵} \\ \hline
名称 & 生命力 & 攻撃力 & 防御力 \\ \hline
\hline
スライム Lv1 & 4 & 1 & 1 \\ \hline
スライム Lv2 & 5 & 1 & 1 \\ \hline
スライム Lv3 & 5 & 1 & 2 \\ \hline
スライム Lv4 & 6 & 1 & 2 \\ \hline
スライム Lv5 & 7 & 2 & 2 \\ \hline
スライム Lv6 & 8 & 2 & 2 \\ \hline
スライム Lv7 & 9 & 3 & 3 \\ \hline
スライム Lv8 & 10 & 3 & 3 \\ \hline
スライム Lv9 & 11 & 3 & 4 \\ \hline
スライム Lv10 & 12 & 3 & 4 \\ \hline
ゴブリン Lv1 & 3 & 2 & 1 \\ \hline
ゴブリン Lv2 & 4 & 3 & 1 \\ \hline
ゴブリン Lv3 & 5 & 4 & 1 \\ \hline
ゴブリン Lv4 & 6 & 5 & 1 \\ \hline
ゴブリン Lv5 & 7 & 6 & 2 \\ \hline
ゴブリン Lv6 & 8 & 7 & 2 \\ \hline
ゴブリン Lv7 & 9 & 8 & 2 \\ \hline
ゴブリン Lv8 & 10 & 9 & 2 \\ \hline
ゴブリン Lv9 & 11 & 10 & 3 \\ \hline
ゴブリン Lv10 & 12 & 11 & 3 \\ \hline
ドラゴン & 50 & 8 & 3 \\ \hline 
\hline \hline
\multicolumn{4}{|c|}{アイテム}\\ \hline
名称 & \multicolumn{3}{c|}{効果} \\ \hline
\hline
新しい鎧 & \multicolumn{3}{l|}{最大生命力を5増加} \\ \hline
新しい剣 & \multicolumn{3}{l|}{攻撃力を2増加} \\ \hline
新しい盾 & \multicolumn{3}{l|}{防御力を2増加} \\ \hline
爆弾 & \multicolumn{3}{l|}{爆弾所持数を1増加} \\ \hline
薬草 & \multicolumn{3}{l|}{薬草所持数を1増加} \\ \hline
宿屋 & \multicolumn{3}{l|}{生命力を10回復} \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{学習におけるディレクターの構造}
この手法の手順(\ref{logic_greedy})で示した通り記録された、ディレクターがゲーム中で使用した入出力の値のうち、
選ばれた選択肢(この手法では最も大きい値2つ)を、算出されたスコア値に変更し、誤差修正関数にかけることで学習を行う。

\subsubsection{スコアの算出方法}
プレイヤーが死亡するまでに到達したゲームの進行度10回分の平均と、目標値の差をスコアとする。
この手法では目標の値を14としている(全20階のうち14階目の選択肢でちょうど力尽きる難易度に調整するため)。
このときスコアは-14から0の範囲となり、0が最高である。
例えば、力尽きた階層の平均が10階だった場合、スコアは-4である。

\subsubsection{Fixed Target Q-Network}
ネットワークを毎回更新すると、いつまでも収束しないという事態が発生する可能性がある。
そのため、学習はスコアの取得10回毎にのみ行う。それまで教師データは保存される。

\subsubsection{εに基づくランダムなアクション}
ε-グリーディ法に倣い、学習初期は学習したものではなく乱数を出力に用いる。
様々な出力を試行することで、より高いスコアの出力を発見する狙いがある。
このランダムな試行はεの確率で行われる。εの初期値は1.0(100\%)、学習が行われる(スコア取得10回)毎に0.05(5\%)減算される。

\subsubsection{結果}
結果、提示される選択肢はゲームを通して同じものばかりになり、
それに伴い学習に応じて結果が極端に上下するようになった。
ニューラルネットワークでは、入力層への値ひとつが多少変化したところで出力が劇的に変化するような学習は
困難であるため、ゲームを通して同じ選択肢ばかりになったものと考察される。

\begin{table}[hpbt]
\caption{一般的なε-グリーディ法を使用した方法における平均到達階層とスコア}
\label{figure_director_v7}
\begin{center}
\includegraphics[scale=0.7]{plot_director_v7.eps}
\end{center}
\end{table}

\newpage

\subsection{重み付きランダムを採用した方法}\label{random-method}
先の方法で取った手法に加えて、選択肢を提示する方法を変更する。
出力の最高値2つではなく、出力値を重みとしたランダムな抽出で選択肢を提示する方法をとった。

\subsubsection{この手法の手順} \label{logic_random}

\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、ディレクションと最終到達階層を記録し、それらを基にスコアを算出する。
\item 1.と2.を10回行い、データを溜める(最大で2000データ、最大10階*10プレイ*スコア算出10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ゲーム中におけるディレクターの構造}
入力値7個、出力値17個の、Chainerを利用したニューラルネットワーク。
入力値は、プレイヤーのステータス(\pageref{tableBattleStatus}ページ,表\ref{tableBattleStatus}を参照)の値6つに、現在のプレイヤー位置の値を合わせた実数値7個。
出力値は、出現する全ての敵11種類と、出現する全アイテム6種類、それぞれの価値を決める実数値17個。
それらのステータスは先の手法に同じ。表\ref{tableGameItems}(\pageref{tableGameItems}ページ)を参照。
出力値を重みとし、重み付きランダムで出現する敵またはアイテムを2つ選ぶ。

\subsubsection{学習におけるディレクターの構造}
この手法の手順(\ref{logic_random})で示した通り記録された、ディレクターがゲーム中で使用した入出力の値のうち、
選ばれた選択肢(重み付きランダムで選ばれていた2つ)を、算出されたスコア値に変更し、誤差修正関数にかけることで学習を行う。

\subsubsection{結果}
3000回目前後の学習以降、スコアが安定して-5以上出る結果となった。
選択肢を直接決めるのではなく、何が出やすいのか傾向を決めさせることで、選択毎に変化が生じるゲームとなった。
またεによらない通常の学習でもランダム性を持たせることで、現状よりも良い選択を発見する助けにもなっているのでは
ないかと考察できる。
しかし、学習3000回目以降も継続して学習を続けても-5より高いスコアを安定して出すことは叶わなかった。この原因は
学習のし過ぎで高いスコアの状態で難易度が安定していないものと仮定された。

\begin{table}[hpbt]
\caption{重み付きランダムを採用した方法における平均到達階層とスコア}
\label{figure_random_v7}
\begin{center}
\includegraphics[scale=0.7]{plot_random_v7.eps}
\end{center}
\end{table}

\newpage

\subsection{学習に応じて学習率を低下させていく方法}
先の手法を改良し、直近のスコアから学習の進み具合を見て、学習率を低下させる方法をとった。
先の図において学習3000回目以降、目標付近で揺れ動いているaverage(橙の線)を目標に収束させることが狙いである。

\subsubsection{この手法の手順} 
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、AIディレクターの出力(以下、ディレクションとする)と、最終到達階層を記録し、それらを基にスコアを算出する。
\item 1.と2.を10回行い、データを溜める(最大で2000データ、最大20階*10プレイ*スコア算出10回)。
\item 3.のスコアに応じて、学習率lrを減少させる。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ディレクターの構造}
この手法は、\ref{random-method}(\pageref{random-method}ページ)の重み付きランダムを採用した方法を改良したものであり、
ディレクターの仕組みは、学習率の変更が為される箇所を除き\ref{random-method}に同じである。

\subsubsection{学習率の変更}
スコアの取得10回毎に行われる学習の直前に、その10回のスコアの平均$avg$に応じて、学習率$lr$を以下のように変更する。
\begin{math}
avg>-5のとき
lr=0.01/(10^(\frac{5+avg}{2}))
avg<=-5のとき
lr=0.01
\end{math}

\begin{table}[hpbt]
\caption{学習に応じて学習率を低下させていく方法における平均到達階層とスコア}
\label{figure_random_v10}
\begin{center}
\includegraphics[scale=0.7]{plot_random_v10.eps}
\end{center}
\end{table}

\subsubsection{結果}
結果、狙い通りにはならず、スコア-5以上のスコアを安定して出すことはできなかった。
しかし、高いスコアが先の手法よりも早く出るようになった。
-5以上のスコアが多く出てはいるが、学習が不足しているのか、しばしば低いスコアが出るという不安定なものとなった。

\subsection{出現する敵を動的に生成する方法}
-5より高いスコアが出ない原因として、一段階易しい選択肢を提示したときに易しすぎ、逆に一段階難しい選択肢を提示したときに難しすぎる、という「帯に短し襷に長し」状態になっているがために振れ幅が大きくなっているという仮説を立てた。
これまでの手法では、あらかじめ静的に決められた強さを持つ敵を十数種類から選んでいた。
これを動的なものに変更する。

\subsubsection{この手法の手順} \label{logic_enemygen}
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、ディレクションと最終到達階層を記録し、それらを基にスコアを算出する。
\item 2.を10回行い、データを溜める(最大で1000データ、最大10階*10プレイ*スコア算出10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ディレクターの仕組み}
敵とアイテムの2択をディレクションとして出力する。

敵を生成するために、ニューラルネットワークを2つ使用する。
片方は、出現する3種類の敵から決める(実数値3つを出力し、そのうち最大のもの)。
もう片方は、敵の能力の倍率を決める実数値1つを出力する。
(ここにイメージ図)

更にもう1つのニューラルネットワークを使用し、出現するアイテムを決める(実数値6つを出力し、そのうち最大のもの)。

この手法で登場する敵およびアイテムは、表\ref{tableGameItems2}(\pageref{tableGameItems2}ページ)の通り。
\begin{table}[hpbt]
\caption{ゲーム中に出現する敵およびアイテムのリスト}
\label{tableGameItems2}
\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{4}{|c|}{敵 (xは第2ニューラルネットワークの出力値)} \\ \hline
名称 & 生命力 & 攻撃力 & 防御力 \\ \hline
\hline
スライム & 4*x & 1*x & 2*x \\ \hline
ゴブリン & 3*x & 2*x & 1*x \\ \hline
ドラゴン & 4*x & 2*x & 2*x \\ \hline \hline \hline
\multicolumn{4}{|c|}{アイテム}\\ \hline
名称 & \multicolumn{3}{c|}{効果} \\ \hline
\hline
新しい鎧 & \multicolumn{3}{l|}{最大生命力を5増加} \\ \hline
新しい剣 & \multicolumn{3}{l|}{攻撃力を2増加} \\ \hline
新しい盾 & \multicolumn{3}{l|}{防御力を2増加} \\ \hline
爆弾 & \multicolumn{3}{l|}{爆弾所持数を1増加} \\ \hline
薬草 & \multicolumn{3}{l|}{薬草所持数を1増加} \\ \hline
宿屋 & \multicolumn{3}{l|}{生命力を10回復} \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{結果}
スコアが-3前後となり、これまでの手法よりも高いスコアとなった。
しかし、これ以上続けてもスコアの向上は認められなかった。
また、この手法では学習1回相当のスコア取得に要するプレイ時間が長くなってしまい、規模の大きいゲームの学習には向かないものと思われる。
\begin{table}[hpbt]
\caption{出現する敵を動的に生成する方法における平均到達階層とスコア}
\label{mul_table}
\begin{center}
\includegraphics[scale=0.7]{plot_multiply.eps}
\end{center}
\end{table}

\section{おわりに}
ゲームのバランス調整には、あらかじめ設定された敵の中から選択させる離散的な手法よりも、敵を動的に生成する連続的な手法のほうが適していることが分かった。
また今回の場合、学習のためにゲームをプレイする回数が少なくとも20,000回程度、万全を期すなら100,000回以上必要であることが分かった。
今回のゲームは今日遊ばれているようなゲームと比べてかなり単純なものであるため、実際のゲームで強化学習を使いゲームバランスを調整する場合、さらに多くの試行回数が必要であることが予測される。
これを人力で行うことは現実的ではなく、自動的にプレイする機構が必要であると考えらる。また、それができるゲームの種類は限られてくるものと推測される。

\begin{thebibliography}{9}
\bibitem{}
Sebastian Raschka, Vahid Mirjalili, 株式会社クイーブ: [第3版]Python機械学習プログラミング 達人データサイエンティストによる理論と実践, 株式会社インプレス, 2020年10月21日
\bibitem{}
ishizakiiii: OpenAI Gym入門
\\yyyy年mm月dd日最終確認
\\\url{https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16}
\bibitem{}
ishizakiiii: DQN（Deep Q Network）を理解したので、Gopherくんの図を使って説明
\\yyyy年mm月dd日最終確認
\\\url{https://qiita.com/ishizakiiii/items/5eff79b59bce74fdca0d#q-learning}
\bibitem{}
icoxfog417, Takahiro Kubo: ゼロからDeepまで学ぶ強化学習
\\yyyy年mm月dd日最終確認
\\\url{https://qiita.com/icoxfog417/items/242439ecd1a477ece312}
\\\url{https://www.slideshare.net/takahirokubo7792/python-openai-gym}
\bibitem{}
Mark Brown, Game Maker's Toolkit: What Makes Good AI?
\\yyyy年mm月dd日最終確認
\\\url{https://www.youtube.com/watch?v=9bbhJi0NBkk&t=646s}
\bibitem{}
Michael Booth, Valve: The AI Systems of Left 4 Dead
\\yyyy年mm月dd日最終確認
\\\url{https://steamcdn-a.akamaihd.net/apps/valve/2009/ai_systems_of_l4d_mike_booth.pdf}
\bibitem{}
Mark Brown, Game Maker's Toolkit: What Capcom Didn't Tell You About ResidentEvil4
\\yyyy年mm月dd日最終確認
\\\url{https://www.youtube.com/watch?v=zFv6KAdQ5SE}
\bibitem{}
渡辺 修司, 中村 彰憲: なぜ人はゲームにはまるのか
\\yyyy年mm月dd日最終確認
\\\url{https://online.sbcr.jp/2014/06/003741.html}
\end{thebibliography}

\appendix
\def\thesection{付録}
\section{ソースコード}
当研究で使用したゲームおよび学習システムのソースコードは、GitHub上で公開されている。\\
\url{https://github.com/KaichiHamaishi/aiDirectedRPG}

\end{document}
