\documentclass{jarticle}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{ascmac}
\usepackage{url}



\title{強化学習を利用したAIディレクターの制作}
\author{濱石海地}
\date{2021年11月2日}

\setlength{\textwidth}{16truecm}
\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\begin{document}

%\maketitle
%\clearpage
\begin{titlepage}
    \begin{center}

        
        \vspace*{50truept}

        {\Huge 卒業論文}

        \vspace{50truept}

        {\Huge 強化学習を利用したAIディレクターの制作} 

        \vspace{130truept}

        {\huge 甲南大学　知能情報学部}

        \vspace{10truept}

        {\huge 知能情報学科}

        \vspace{10truept}

        {\huge 11771083}

        \vspace{10truept}

        {\huge 濱石 海地}

        \vspace{80truept}

        {\huge 2021年11月2日} 
     
       \vspace{10truept}     
	  {\huge 指導教授　田中雅博} 
    \end{center}
\end{titlepage}

\tableofcontents
\clearpage

\section{はじめに}
ゲームにおいて難易度は, それ一つでゲームの面白さを左右する重要な要素であり, ゲーム開発において最も工数をかけるべき箇所のひとつである。
一般的にはゲームの進行と共にプレイヤーの腕が向上するため, ゲームが進行するほど難易度も上がるよう調整される場合がほとんどである。
しかし, プレイヤーの腕や, 面白く感じる難易度は人それぞれであり, またその調整は試行錯誤を繰り返すなどして調整する必要があり, 工数の多さはゲーム開発者にとって大きな負担となる。
それを解決するべく, 本研究では「AIディレクター」に着目した。AIディレクターとは, Valve Software社開発のシューティングゲーム「Left 4 Dead」に使用されている難易度調整システムの呼称であり, それはプレイヤー達の体力や所持品, 敵との戦い方からプレイヤーの状態を推定し, 敵の配置などを調整している\cite{5, 6}。呼称や手法は異なるものの, プレイヤーの状況によって難易度を秘密裏に調整するシステムは「バイオハザード4」など他のゲームにも見られる\cite{7, 8}。
これら「AIディレクター」は, 多くはルールベースによって作られていると考えられる。本研究はこれを機械学習を用いて作成することで, ユーザーそれぞれに最適な難易度を提供すると共に, 開発者の負担を軽減することの実現可能性を探るものである。

%アピールする新規性/有用性:
%従来のAIディレクターはルールベースである。
%各々のプレイヤーに対して適切な難易度を提供し, 娯楽性を最大にする。
%ゲーム制作者が難易度調整の工数を削減できる。


\section{研究の概要}
本研究のために作成されたオリジナルのゲームにおいて, 登場する敵やアイテムの出現を強化学習AIに制御させる。

\subsection{ゲームの仕様}
Pythonのコマンドライン上で動作する, 簡素なRPGを作成した。

\subsubsection{ゲーム全体の流れ}
以下の流れを繰り返すことで進行する。
これを所定の回数(10回あるいは20回)繰り返した後, プレイヤーが生存しているならゲームクリアとする。
\begin{enumerate}
\item ディレクター(強化学習AI)によって2通りの行先が提示されるので, プレイヤーはそれを選ぶ。行先に何があるかは表示されている。
\item 選んだ行先に敵がいるなら, その敵と戦闘を行う。アイテムがあるなら, それを獲得する。
\end{enumerate}
図\ref{playersFlow}を参照のこと。

\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.3]{playersFlow.eps}
\end{center}
\caption{ゲームを遊ぶ流れのイメージ図}
\label{playersFlow}
\end{figure}

\subsubsection{戦闘の流れ}
戦闘ではプレイヤーと敵が, どちらかの生命力が0以下になるまで, 
「プレイヤー, 敵, プレイヤー, 敵…」の順番で交互に行動する。
戦闘における行動は, 表\ref{tableBattleAction}に示すもののうち1つを選んで行う。
\begin{table}[hpbt]
\caption{戦闘における行動}
\label{tableBattleAction}
\begin{center}
\begin{tabular}{cl}
\\ \hline
攻撃 & 相手に自身の攻撃力と同値のダメージを与える(生命力を減らす)。 \\ \hline
防御 & 相手から受けるダメージを, 自身の防御力の値だけダメージを減少させる。 \\ \hline
薬草を使用 & 薬草所持数を1消費し, 自身の生命力を全回復。 \\ \hline
爆弾を使用 & 爆弾所持数を1消費し, 攻撃力2倍の攻撃を行う。 \\ \hline
\end{tabular}
\end{center}
\end{table}
プレイヤーの行動はコンソールからの手動入力, あるいはプログラムによる自動入力で選ばれる。
敵の行動は, 敵自身が持つ数値が示す確率に応じてランダムに選ばれる(\ref{class_desc}を参照)。

\subsubsection{プレイヤーおよび敵のステータス(能力値)}
プレイヤーおよび敵は, 表\ref{tableBattleStatus}に示す値をステータスとして持ち, それらは戦闘中の行動に影響する。
プレイヤーは6つ(生命力,最大生命力,攻撃力,防御力,薬草所持数,爆弾所持数)の, 敵は4つ(生命力,最大生命力,攻撃力,防御力)の値を持つ。
\begin{table}[hpbt]
\caption{プレイヤーおよび敵のステータス(能力値)}
\label{tableBattleStatus}
\begin{center}
\begin{tabular}{cl}
\\ \hline
生命力 & これが0になったら死亡。アイテム等により回復する。 \\ \hline
最大生命力 & 生命力はこの値を超えて回復しない。アイテム拾得により増加。 \\ \hline
攻撃力 & 戦闘において敵に与えるダメージ量に影響。\\
 & 戦闘に勝利するか, 特定のアイテム拾得により増加。 \\ \hline
防御力 & 戦闘において防御したとき, この値だけ受けるダメージ量が減る。\\
 & 戦闘に勝利するか, 特定のアイテム拾得により増加。  \\ \hline
薬草所持数 & プレイヤー専用。戦闘中に消費して生命力を回復できる。\\
 & アイテム拾得により増加。  \\ \hline
爆弾所持数 & プレイヤー専用。戦闘中に消費して攻撃力2倍の攻撃ができる。\\
 & アイテム拾得により増加。 \\ \hline
\end{tabular}
\end{center}
\end{table}
\subsection{ソースコードの構造および各クラスの仕様}\label{class_desc}
ゲームを遊ぶプレイヤーと, ゲームに登場する敵・アイテムを操作するディレクターは, 該当部分のクラスを差し替えることで, 
他のゲーム部分を編集することなく作り変えることが可能である。
\begin{itemize}
\item mainクラス\\
学習を始める際に初めに実行されるクラスである。学習のためにゲームを複数回実行する処理や, ゲームの結果をディレクターに渡す処理を持つ。
\item gameクラス\\
関数としてゲームを実行し, 返り値として最終的なゲームの進行度をmainクラスに渡す。ゲーム中はディレクターが提示した行先の選択肢をプレイヤーに渡す処理, プレイヤーと敵を戦わせる処理を行う。
\item characterクラス\\
playerクラスおよびenemyクラスの親クラスであり, インタフェースとして機能する。表\ref{tableBattleStatus}に示す値のうち, プレイヤーと敵に共通する4つの値を持つ。
また戦闘を行うにあたって必要な, 戦闘における行動を選ぶ関数や, ダメージによって生命力を減少させる関数を持つ。
\item enemyクラス\\
ゲーム中における敵。コンストラクタで設定可能な値として行動傾向を示す実数値リストを持ち, 戦闘中における各種行動(表\ref{tableBattleAction})のうち, それを選ぶ確率を示す。
\item playerクラス\\
ゲーム中におけるプレイヤー(主人公)。characterクラスに加えて, ディレクターの提示した選択肢を選ぶ関数と, 薬草所持数と爆弾所持数を示す2つの整数値を持つ。
標準では, 選択肢と戦闘中の行動はコマンドライン上で手動指定する。
\item ルールベース自動プレイヤークラス\\
playerクラスの子クラス。コマンドラインによる手動選択の代わりに, 条件分岐により自動的に選択を行う。学習中はこちらを使用する。
\item treasureクラス\\
ゲーム中でプレイヤーが入手するアイテム。プレイヤーの各ステータス値をいくら増減させるかの値を持つ。
\item directorクラス\\
ディレクター。ディレクションとして選択肢を返す関数, 学習を行うためにゲームの結果を入力する返り値なしの関数の2つの関数を持つ。
本研究における各手法は, このクラスの子クラスとして作成する。
\end{itemize}
これらクラスの関係性については、図\ref{UML_class}に示すクラス図を参照のこと。

\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.25]{aiDirectedRPG.eps}
\end{center}
\caption{本研究に使用したプログラムのクラス図}
\label{UML_class}
\end{figure}

\newpage

\section{学習の方法と結果}
以上を踏まえ, ディレクターを学習させることでゲームの難易度を最適なものとすることを試みる。
本研究における最適な難易度とは, ルールベースで自動的にプレイするするプレイヤーが, ゲームを7割進行させた所でちょうどゲームオーバーとなるような難易度としている。

\subsection{各手法に共通する流れ}
ディレクターに適切な難易度を学習させるためには, 難易度を測る指標が必要となる。
本研究ではディレクターの調整した難易度を測るために, 自動でゲームを複数回プレイし, それらのプレイでどこまで進めることができたか(以降, 最終到達階層と記す)の平均を求め, これを難易度の指標としている。図\ref{learnFlow}に, この流れのイメージ図を表している。
\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.3]{learnFlow.eps}
\end{center}
\caption{学習全体の流れ。図中の数値は研究初期の一例}
\label{learnFlow}
\end{figure}
ディレクタークラスには2つのグローバル関数があり, 一つがディレクションを返す関数, もう一つが学習を行わせる関数である。

ディレクションを返す関数には, 引数としてプレイヤーの各種能力, 出現する敵やアイテムのリストを渡す。返り値として出現させるべき敵およびアイテムのリストが渡される。
内部ではプレイヤーの能力値を入力値に, 出現する敵やアイテムの価値を出力とするニューラルネットワークの前向き計算が行われる。それと同時にディレクションをクラス内に保存している。
このニューラルネットワークの入出力値について, 図\ref{directorMechanics}に表している。

学習を行わせる関数には, 引数としてゲームの平均到達階層を渡す。返り値は無い。
引数を基にディレクタークラス側で保存されていたディレクションのスコアを計算し, このスコアを基にニューラルネットワークの
%誤差修正を行う。
誤差逆伝播による最適化を行う。

\subsubsection{教師データの形式}
ゲーム中のディレクターは, ニューラルネットワークに対する入力値とその出力値を記憶する。スコアが算出された際には, 出力値のうち実際にゲームで提示した選択肢に対応するものをスコアに置き換え, これを教師データにして学習を行う。
\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.3]{directorMechanics.eps}
\end{center}
\caption{ディレクターの仕組み}
\label{directorMechanics}
\end{figure}

\subsection{一般的なε-グリーディ法を使用した方法} \label{section_greedy}
まず最初に一般的な強化学習の手法に見られる\cite{1,2,3,4}ような, 
出力値のうち最も高い値2つを選択肢として提示する方法をとった。

\subsubsection{手法の手順} \label{logic_greedy}
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において, AIディレクターの出力(以下, ディレクションとする)と, 最終到達階層を記録し, それらを基にスコアを算出する。
\item 1.と2.を10回行い, データを溜める(最大で2000データ, 10プレイ*最大20階*10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し, 1.に戻る。
\end{enumerate}

\subsubsection{ゲーム中におけるディレクターの構造}
入力値7個, 出力値17個の, Chainerを利用したニューラルネットワーク。
入力値は, プレイヤーのステータス(表\ref{tableBattleStatus}を参照)の値6つに, 現在のプレイヤー位置の値を合わせた7個がある。
出力値は, 出現する全ての敵11種類と, 出現する全てのアイテム6種類, それぞれの価値を決める実数値の17個がある。それらのステータスについては表\ref{tableGameItems}を参照。
出力値のうち, 最も大きい値2つが示している敵またはアイテムがゲームに出現する。

\begin{table}[hpbt]
\caption{ゲーム中に出現する敵およびアイテムのリスト}
\label{tableGameItems}
\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{4}{|c|}{敵} \\ \hline
名称 & 生命力 & 攻撃力 & 防御力 \\ \hline
\hline
スライム Lv1 & 4 & 1 & 1 \\ \hline
スライム Lv2 & 5 & 1 & 1 \\ \hline
スライム Lv3 & 5 & 1 & 2 \\ \hline
スライム Lv4 & 6 & 1 & 2 \\ \hline
スライム Lv5 & 7 & 2 & 2 \\ \hline
スライム Lv6 & 8 & 2 & 2 \\ \hline
スライム Lv7 & 9 & 3 & 3 \\ \hline
スライム Lv8 & 10 & 3 & 3 \\ \hline
スライム Lv9 & 11 & 3 & 4 \\ \hline
スライム Lv10 & 12 & 3 & 4 \\ \hline
ゴブリン Lv1 & 3 & 2 & 1 \\ \hline
ゴブリン Lv2 & 4 & 3 & 1 \\ \hline
ゴブリン Lv3 & 5 & 4 & 1 \\ \hline
ゴブリン Lv4 & 6 & 5 & 1 \\ \hline
ゴブリン Lv5 & 7 & 6 & 2 \\ \hline
ゴブリン Lv6 & 8 & 7 & 2 \\ \hline
ゴブリン Lv7 & 9 & 8 & 2 \\ \hline
ゴブリン Lv8 & 10 & 9 & 2 \\ \hline
ゴブリン Lv9 & 11 & 10 & 3 \\ \hline
ゴブリン Lv10 & 12 & 11 & 3 \\ \hline
ドラゴン & 50 & 8 & 3 \\ \hline 
\hline \hline
\multicolumn{4}{|c|}{アイテム}\\ \hline
名称 & \multicolumn{3}{c|}{効果} \\ \hline
\hline
新しい鎧 & \multicolumn{3}{l|}{最大生命力を5増加} \\ \hline
新しい剣 & \multicolumn{3}{l|}{攻撃力を2増加} \\ \hline
新しい盾 & \multicolumn{3}{l|}{防御力を2増加} \\ \hline
爆弾 & \multicolumn{3}{l|}{爆弾所持数を1増加} \\ \hline
薬草 & \multicolumn{3}{l|}{薬草所持数を1増加} \\ \hline
宿屋 & \multicolumn{3}{l|}{生命力を10回復} \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{スコアの算出方法}
プレイヤーが死亡するまでに到達したゲームの進行度10回分の平均と, 目標値の差をスコアとする。
この手法では目標の値を14としている(全20階のうち14階目の選択肢でちょうど力尽きる難易度に調整するため)。
このときスコアは-14から0の範囲となり, 0が最高である。
例えば, 力尽きた階層の平均が10階だった場合, スコアは-4である。

\subsubsection{Fixed Target Q-Network}
ネットワークを毎回更新すると, いつまでも収束しないという事態が発生する可能性がある。
そのため, 学習はスコアの取得10回毎にのみ行う。それまで教師データは保存される。

\subsubsection{εに基づくランダムなアクション}
ε-グリーディ法に倣い, 学習初期は学習したものではなく乱数を出力に用いる。
様々な出力を試行することで, より高いスコアの出力を発見する狙いがある。
このランダムな試行はεの確率で行われる。εの初期値は1.0(100\%), 学習が行われる(スコア取得10回)毎に0.05(5\%)減算される。

\subsubsection{結果}
提示される選択肢はゲームを通して同じものばかりになり, それに伴い学習に応じて結果が極端に上下するようになった。
ニューラルネットワークでは, 入力層への値ひとつが多少変化したところで出力が劇的に変化するような学習は
困難であるため\cite{9}, ゲームを通して同じ選択肢ばかりになったものと考察される。
ゲームの結果とスコアの遷移は図\ref{figure_director_v7}の通り。

\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.7]{plot_director_v7.eps}
\end{center}
\caption{一般的なε-グリーディ法を使用した方法における平均到達階層とスコア}
\label{figure_director_v7}
\end{figure}

\newpage

\subsection{重み付きランダムを採用した方法}\label{random-method}
先の方法で取った手法に加えて, 選択肢を提示する方法を変更する。
出力の最高値2つではなく, 出力値を重みとしたランダムな抽出で選択肢を提示する方法をとった。

\subsubsection{この手法の手順} \label{logic_random}

\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において, ディレクションと最終到達階層を記録し, それらを基にスコアを算出する。
\item 1.と2.を10回行い, データを溜める(最大で2000データ, 最大10階*10プレイ*スコア算出10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し, 1.に戻る。
\end{enumerate}

\subsubsection{ゲーム中におけるディレクターの構造}
入力値7個, 出力値17個の, Chainerを利用したニューラルネットワーク。
入力値は, プレイヤーのステータス(表\ref{tableBattleStatus}を参照)の値6つに, 現在のプレイヤー位置の値を合わせた実数値7個。
出力値は, 出現する全ての敵11種類と, 出現する全アイテム6種類, それぞれの価値を決める実数値17個。
それらのステータスは先の手法に同じ。表\ref{tableGameItems}を参照。
出力値を重みとし, 重み付きランダムで出現する敵またはアイテムを2つ選ぶ。

\subsubsection{学習におけるディレクターの構造}
この手法の手順(\ref{logic_random})で示した通り記録された, ディレクターがゲーム中で使用した入出力の値のうち, 
選ばれた選択肢(重み付きランダムで選ばれていた2つ)を, 算出されたスコア値に変更し, 誤差修正関数にかけることで学習を行う。

\subsubsection{結果}
3000回目前後の学習以降, スコアが安定して-5以上出る結果となった。
選択肢を直接決めるのではなく, 何が出やすいのか傾向を決めさせることで, 選択毎に変化が生じるゲームとなった。
またεによらない通常の学習でもランダム性を持たせることで, 現状よりも良い選択を発見する助けにもなっているのではないかと考察できる。
しかし, 学習3000回目以降も学習を続けても, -5より高いスコアを安定して出すことは叶わなかった。この原因は学習のし過ぎで高いスコアの状態で難易度が安定していないものと仮定された。
ゲームの結果とスコアの遷移は図\ref{figure_random_v7}の通り。

\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.7]{plot_random_v7.eps}
\end{center}
\caption{重み付きランダムを採用した方法における平均到達階層とスコア}
\label{figure_random_v7}
\end{figure}

\newpage

\subsection{学習に応じて学習率を低下させていく方法}\label{section_random10}
\ref{random-method}章の手法を改良し, 直近のスコアから学習の進み具合を見て, 学習率を低下させる方法をとった。
図\ref{figure_random_v7}において学習3000回目以降, 目標付近で揺れ動いているaverage(橙の線)を目標に収束させることが狙いである。

\subsubsection{この手法の手順} 
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において, AIディレクターの出力(以下, ディレクションとする)と, 最終到達階層を記録し, それらを基にスコアを算出する。
\item 1.と2.を10回行い, データを溜める(最大で2000データ, 最大20階*10プレイ*スコア算出10回)。
\item 3.のスコアに応じて, 学習率lrを減少させる。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し, 1.に戻る。
\end{enumerate}

\subsubsection{ディレクターの構造}
この手法は, \ref{random-method}章の重み付きランダムを採用した方法を改良したものであり, 
ディレクターの仕組みは, 学習率の変更が為される箇所を除き\ref{random-method}章と同様である。

\subsubsection{学習率の変更}
スコアの取得10回毎に行われる学習の直前に, その10回のスコアの平均$avg$に応じて学習率$lr$を変更する。$avg>-5$のときは式(\ref{lr_math_1}), $avg<=-5$のときは式(\ref{lr_math_2})。
\begin{equation}
\label{lr_math_1}
lr=0.01/10^{\frac{5+avg}{2}}
\end{equation}
\begin{equation}
\label{lr_math_2}
lr=0.01
\end{equation}

\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.7]{plot_random_v10.eps}
\end{center}
\caption{学習に応じて学習率を低下させていく方法における平均到達階層とスコア}
\label{figure_random_v10}
\end{figure}

\subsubsection{結果}
結果, 狙い通りにはならず, スコア-5以上のスコアを安定して出すことはできなかった。
しかし, 高いスコアが先の手法よりも早く出るようになった。
-5以上のスコアが多く出てはいるが, 学習が不足しているのか, しばしば低いスコアが出るという不安定なものとなった。
ゲームの結果とスコアの遷移は図\ref{figure_random_v10}の通り。

\subsection{出現する敵を動的に生成する方法}\label{section_enemygen}
-5より高いスコアが出ない原因として, 一段階易しい選択肢を提示したときに易しすぎ, 逆に一段階難しい選択肢を提示したときに難しすぎる, という「帯に短し襷に長し」状態になっているがために振れ幅が大きくなっているという仮説を立てた。
これまでの手法では, あらかじめ静的に決められた強さを持つ敵を十数種類から選んでいた。
これを動的なものに変更する。

\subsubsection{手法の手順} \label{logic_enemygen}
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において, ディレクションと最終到達階層を記録し, それらを基にスコアを算出する。
\item 2.を10回行い, データを溜める(最大で1000データ, 最大10階*10プレイ*スコア算出10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し, 1.に戻る。
\end{enumerate}

\subsubsection{ディレクターの仕組み}
敵とアイテムの2択をディレクションとして出力する。

敵を生成するために, ニューラルネットワークを2つ使用する。
片方は, 出現する3種類の敵から決める(実数値3つを出力し, そのうち最大のもの)。
もう片方は, 敵の能力の倍率を決める実数値1つを出力する。
この手順は, 図\ref{multiplyFlow}に表している。
\begin{figure}[hpbt]
\label{multiplyFlow}
\begin{center}
\includegraphics[scale=0.3]{multiplyFlow.eps}
\end{center}
\caption{出現する敵を動的に生成する方法}
\end{figure}

更にもう1つのニューラルネットワークを使用し, 出現するアイテムを決める(実数値6つを出力し, そのうち最大のもの)。

この手法で登場する敵およびアイテムは, 表\ref{tableGameItems2}の通り。
\begin{table}[hpbt]
\caption{ゲーム中に出現する敵およびアイテムのリスト}
\label{tableGameItems2}
\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{4}{|c|}{敵 (xは第2ニューラルネットワークの出力値)} \\ \hline
名称 & 生命力 & 攻撃力 & 防御力 \\ \hline
\hline
スライム & 4*x & 1*x & 2*x \\ \hline
ゴブリン & 3*x & 2*x & 1*x \\ \hline
ドラゴン & 4*x & 2*x & 2*x \\ \hline \hline \hline
\multicolumn{4}{|c|}{アイテム}\\ \hline
名称 & \multicolumn{3}{c|}{効果} \\ \hline
\hline
新しい鎧 & \multicolumn{3}{l|}{最大生命力を5増加} \\ \hline
新しい剣 & \multicolumn{3}{l|}{攻撃力を2増加} \\ \hline
新しい盾 & \multicolumn{3}{l|}{防御力を2増加} \\ \hline
爆弾 & \multicolumn{3}{l|}{爆弾所持数を1増加} \\ \hline
薬草 & \multicolumn{3}{l|}{薬草所持数を1増加} \\ \hline
宿屋 & \multicolumn{3}{l|}{生命力を10回復} \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{結果}
スコアが-3前後となり, これまでの手法よりも高いスコアとなった。
しかし, これ以上続けてもスコアの向上は認められなかった。
また, この手法ではニューラルネットワークを3つ使用するため処理時間が長くなり、1ゲームのプレイ時間が長くなってしまった(\ref{section_greedy}章〜\ref{section_random10}章の場合1ゲームに最長約0.016秒, この手法の場合最長約0.245秒)。そのため規模の大きいゲームの学習には向かないものと思われる。ゲームの結果とスコアの遷移は図\ref{mul_table}の通り。
\begin{figure}[hpbt]
\begin{center}
\includegraphics[scale=0.7]{plot_multiply.eps}
\end{center}
\caption{出現する敵を動的に生成する方法における平均到達階層とスコア}
\label{mul_table}
\end{figure}

\section{おわりに}
ゲームのバランス調整には, あらかじめ設定された敵の中から選択させる離散的な手法よりも, 敵を動的に生成する連続的な手法のほうが適していることが分かった。
また今回の場合, 学習のためにゲームをプレイする回数が少なくとも20, 000回程度, 万全を期すなら100, 000回以上必要であることが分かった。
\\今回のゲームは今日遊ばれているようなゲームと比べてかなり単純なものであるため, 実際のゲームで強化学習を使いゲームバランスを調整する場合, さらに多くの試行回数が必要であることが予測される。
これを人力で行うことは現実的ではなく, 自動的にプレイする機構が必要であると考えらる。また, それができるゲームの種類は限られてくるものと推測される。

\begin{thebibliography}{9}
\bibitem{1}
Sebastian Raschka,  Vahid Mirjalili,  株式会社クイーブ: [第3版]Python機械学習プログラミング 達人データサイエンティストによる理論と実践,  株式会社インプレス,  2020年10月21日
\bibitem{2}
ishizakiiii: OpenAI Gym入門
\\yyyy年mm月dd日最終確認
\\\url{https://qiita.com/ishizakiiii/items/75bc2176a1e0b65bdd16}
\bibitem{3}
ishizakiiii: DQN（Deep Q Network）を理解したので, Gopherくんの図を使って説明
\\yyyy年mm月dd日最終確認
\\\url{https://qiita.com/ishizakiiii/items/5eff79b59bce74fdca0d#q-learning}
\bibitem{4}
icoxfog417,  Takahiro Kubo: ゼロからDeepまで学ぶ強化学習
\\yyyy年mm月dd日最終確認
\\\url{https://qiita.com/icoxfog417/items/242439ecd1a477ece312}
\\\url{https://www.slideshare.net/takahirokubo7792/python-openai-gym}
\bibitem{5}
Mark Brown,  Game Maker's Toolkit: What Makes Good AI?
\\yyyy年mm月dd日最終確認
\\\url{https://www.youtube.com/watch?v=9bbhJi0NBkk&t=646s}
\bibitem{6}
Michael Booth,  Valve: The AI Systems of Left 4 Dead
\\yyyy年mm月dd日最終確認
\\\url{https://steamcdn-a.akamaihd.net/apps/valve/2009/ai_systems_of_l4d_mike_booth.pdf}
\bibitem{7}
Mark Brown,  Game Maker's Toolkit: What Capcom Didn't Tell You About ResidentEvil4
\\yyyy年mm月dd日最終確認
\\\url{https://www.youtube.com/watch?v=zFv6KAdQ5SE}
\bibitem{8}
ファミ通: バイオハザード4 解体真書,  スタジオベントスタッフ,  2005年4月2日
%\bibitem{9}
%渡辺 修司,  中村 彰憲: なぜ人はゲームにはまるのか
%\\yyyy年mm月dd日最終確認
%\\\url{https://online.sbcr.jp/2014/06/003741.html}
\bibitem{9}
???: ???, ????年??月??日 
\end{thebibliography}

\appendix
\def\thesection{付録}
\section{ソースコード}
当研究で使用したゲームおよび学習システムのソースコードは, GitHub上で公開されている。\\
\url{https://github.com/KaichiHamaishi/aiDirectedRPG}

\end{document}
