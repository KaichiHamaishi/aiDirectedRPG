\documentclass{jarticle}
\usepackage{inputenc}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{ascmac}



\title{強化学習を利用したAIディレクターの制作}
\author{濱石海地}
\date{2021年10月19日}

\setlength{\textwidth}{16truecm}
\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}

\begin{document}

\maketitle
\clearpage
\tableofcontents
\clearpage

\section{はじめに}
AIディレクターとは、ゲームのおいて進行管理役を務めるシステムである。
ゲームを遊ぶプレイヤーの状態を監視し、ゲームの難易度をリアルタイムで調整を行う。

ゲームにおいて難易度は、それ一つでゲームの面白さを左右する重要な要素であり、ゲーム開発において最も工数をかけるべき箇所のひとつである。
しかしユーザーにとって面白く感じる難易度は人それぞれであり、また工数の多さは開発者にとって大きな負担となる。
\\当研究は「AIディレクター」に着目し、これを機械学習を用いて作成することでユーザーそれぞれに最適な難易度を提供すると共に、開発者の負担を軽減できる可能性を狙ったものである。


\section{研究の概要}
自作のゲームにおいて、ゲームに登場する敵やアイテムの出現を強化学習AIに制御させる。

\subsection{ゲームの仕様}
Pythonのコマンドライン上で動作する、簡素なRPG。

\subsubsection{ゲーム全体の流れ}
以下の流れを繰り返すことで進行する。
これを所定の回数(10回あるいは20回)繰り返した後、プレイヤーが生存しているならゲームクリアとする。
\begin{enumerate}
\item 2通りの行先が提示されるので、プレイヤーはそれを選ぶ。行先に何があるかは表示されている。
強化学習AIはここで何が提示されるのかを選ぶ。
\item 選んだ行先に敵がいるなら、それと戦う。アイテムがあるなら、それを獲得する。
\end{enumerate}
図\ref{playersFlow}(\pageref{playersFlow}ページ)を参照のこと。

\begin{figure}[hbt]
\begin{center}
\includegraphics[scale=0.3]{playersFlow.eps}
\end{center}
\caption{ゲームを遊ぶ流れのイメージ図}
\label{playersFlow}
\end{figure}

\subsubsection{戦闘の流れ}
戦闘ではプレイヤーと敵が、どちらかの生命力が0以下になるまで、
「プレイヤー、敵、プレイヤー、敵…」の順番で交互に行動する。
戦闘における行動は表\ref{tableBattleAction}(\pageref{tableBattleAction}ページ)の通り。
\begin{table}[hbt]
\caption{戦闘における行動}
\label{tableBattleAction}
\begin{tabular}{cl}
\\ \hline
攻撃 & 相手に自身の攻撃力と同値のダメージを与える(生命力を減らす)。 \\ \hline
防御 & 相手から受けるダメージを、自身の防御力の値だけダメージを減少させる。 \\ \hline
薬草を使用 & 薬草所持数を1消費し、自身の生命力を全回復。 \\ \hline
爆弾を使用 & 爆弾所持数を1消費し、攻撃力2倍の攻撃を行う。 \\ \hline
\end{tabular}
\end{table}
プレイヤーの行動はコンソールからの手動入力、あるいはプログラムによる自動入力で選ばれる。
敵の行動は、確率に応じてランダムに選ばれる。

\subsubsection{プレイヤーおよび敵のステータス(能力値)}
\begin{table}[hbt]
\caption{プレイヤーおよび敵のステータス(能力値)}
\label{tableBattleStatus}
\begin{tabular}{cl}
\\ \hline
生命力 & これが0になったら死亡。アイテム等により回復する。 \\ \hline
最大生命力 & 生命力はこの値を超えて回復しない。アイテム拾得により増加。 \\ \hline
攻撃力 & 戦闘において敵に与えるダメージ量に影響。\\
 & 戦闘に勝利するか、特定のアイテム拾得により増加。 \\ \hline
防御力 & 戦闘において防御したとき、この値だけ受けるダメージ量が減る。\\
 & 戦闘に勝利するか、特定のアイテム拾得により増加。  \\ \hline
薬草所持数 & プレイヤー専用。戦闘中に消費して生命力を回復できる。\\
 & アイテム拾得により増加。  \\ \hline
爆弾所持数 & プレイヤー専用。戦闘中に消費して攻撃力2倍の攻撃ができる。\\
 & アイテム拾得により増加。 \\ \hline
\end{tabular}
\end{table}
プレイヤーは6つの、敵は4つの値をステータスとして持ち、それらは戦闘中の行動に影響する。表\ref{tableBattleStatus}(\pageref{tableBattleStatus}ページ)を参照。

\subsection{プラグラム及びソースコードの構造}
ゲームを遊ぶプレイヤーと、ゲームに登場する敵・アイテムを操作するディレクターは、該当部分のクラスを差し替えることで、
他のゲーム部分を編集することなく作り変えることが可能である。
(ここにクラス図)

\newpage

\section{AIディレクターの学習の結果と考察}
以上を踏まえ、
\subsection{一般的なε-グリーディ法を使用した方法}
まず最初に一般的な強化学習の手法に見られるような、
出力値のうち最も高い値2つを選択肢として提示する方法をとった。

\subsubsection{この手法の手順} \label{logic_greedy}
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、AIディレクターの出力(以下、ディレクションとする)と、最終到達階層を記録し、それらを基にスコアを算出する。
\item 2.を10回行い、データを溜める(最大で2000データ、10プレイ*最大20階*10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\begin{figure}[hbt]
\begin{center}
\includegraphics[scale=0.3]{learnFlow.eps}
\end{center}
\caption{学習全体の流れ。図中の数値は研究初期の一例}
\label{learnFlow}
\end{figure}

\subsubsection{ゲーム中におけるディレクターの構造}
\begin{figure}[hbt]
\begin{center}
\includegraphics[scale=0.3]{directorMechanics.eps}
\end{center}
\caption{ディレクターの仕組み}
\label{directorMechanics}
\end{figure}
入力値7個、出力値17個の、Chainerを利用したニューラルネットワーク。
入力値は、プレイヤーのステータス(\pageref{tableBattleStatus}ページ,表\ref{tableBattleStatus}を参照)の値6つに、現在のプレイヤー位置の値を合わせた実数値7個。
出力値は、出現する全ての敵11種類と、出現する全アイテム6種類、それぞれの価値を決める実数値17個。それらのステータスについては表\ref{tableGameItems}(\pageref{tableGameItems}ページ)を参照。
出力値のうち、最も大きい値2つが示している敵またはアイテムがゲームに出現する。

\begin{table}[hbt]
\caption{ゲーム中に出現する敵およびアイテムのリスト}
\label{tableGameItems}
\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{4}{|c|}{敵} \\ \hline
名称 & 生命力 & 攻撃力 & 防御力 \\ \hline
\hline
スライム Lv1 & 4 & 1 & 1 \\ \hline
スライム Lv2 & 5 & 1 & 1 \\ \hline
スライム Lv3 & 5 & 1 & 2 \\ \hline
スライム Lv4 & 6 & 1 & 2 \\ \hline
スライム Lv5 & 7 & 2 & 2 \\ \hline
スライム Lv6 & 8 & 2 & 2 \\ \hline
スライム Lv7 & 9 & 3 & 3 \\ \hline
スライム Lv8 & 10 & 3 & 3 \\ \hline
スライム Lv9 & 11 & 3 & 4 \\ \hline
スライム Lv10 & 12 & 3 & 4 \\ \hline
ゴブリン Lv1 & 3 & 2 & 1 \\ \hline
ゴブリン Lv2 & 4 & 3 & 1 \\ \hline
ゴブリン Lv3 & 5 & 4 & 1 \\ \hline
ゴブリン Lv4 & 6 & 5 & 1 \\ \hline
ゴブリン Lv5 & 7 & 6 & 2 \\ \hline
ゴブリン Lv6 & 8 & 7 & 2 \\ \hline
ゴブリン Lv7 & 9 & 8 & 2 \\ \hline
ゴブリン Lv8 & 10 & 9 & 2 \\ \hline
ゴブリン Lv9 & 11 & 10 & 3 \\ \hline
ゴブリン Lv10 & 12 & 11 & 3 \\ \hline
ドラゴン & 50 & 8 & 3 \\ \hline 
\hline \hline
\multicolumn{4}{|c|}{アイテム}\\ \hline
名称 & \multicolumn{3}{c|}{効果} \\ \hline
\hline
新しい鎧 & \multicolumn{3}{l|}{最大生命力を5増加} \\ \hline
新しい剣 & \multicolumn{3}{l|}{攻撃力を2増加} \\ \hline
新しい盾 & \multicolumn{3}{l|}{防御力を2増加} \\ \hline
爆弾 & \multicolumn{3}{l|}{爆弾所持数を1増加} \\ \hline
薬草 & \multicolumn{3}{l|}{薬草所持数を1増加} \\ \hline
宿屋 & \multicolumn{3}{l|}{生命力を10回復} \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{学習におけるディレクターの構造}
この手法の手順(\ref{logic_greedy})で示した通り記録された、ディレクターがゲーム中で使用した入出力の値のうち、
選ばれた選択肢(この手法では最も大きい値2つ)を、算出されたスコア値に変更し、誤差修正関数にかけることで学習を行う。

\subsubsection{スコアの算出方法}
プレイヤーが死亡するまでに到達したゲームの進行度10回分の平均と、目標値の差をスコアとする。
この手法では目標の値を14としている(全20階のうち14階目の選択肢でちょうど力尽きる難易度に調整するため)。
このときスコアは-14から0の範囲となり、0が最高である。
例えば、力尽きた階層の平均が10階だった場合、スコアは-4である。

\subsubsection{Fixed Target Q-Network}
ネットワークを毎回更新すると、いつまでも収束しないという事態が発生する可能性がある。
そのため、学習はスコアの取得10回毎にのみ行う。それまで教師データは保存される。

\subsubsection{εに基づくランダムなアクション}
ε-グリーディ法に倣い、学習初期は学習したものではなく乱数を出力に用いる。
様々な出力を試行することで、より高いスコアの出力を発見する狙いがある。
このランダムな試行はεの確率で行われる。εの初期値は1.0(100\%)、学習が行われる(スコア取得10回)毎に0.05(5\%)減算される。

\begin{table}[hbt]
\caption{一般的なε-グリーディ法を使用した方法における平均到達階層とスコア}
\label{figure1}
\begin{center}
\includegraphics[scale=0.7]{plot_director_v7.eps}
\end{center}
\end{table}

\subsubsection{結果}
結果、提示される選択肢はゲームを通して同じものばかりになり、
それに伴い学習に応じて結果が極端に上下するようになった。
ニューラルネットワークでは、入力層への値ひとつが多少変化したところで出力が劇的に変化するような学習は
困難であるため、ゲームを通して同じ選択肢ばかりになったものと考察される。

\newpage

\subsection{重み付きランダムを採用した方法}\label{random-method}
先の方法で取った手法に加えて、選択肢を提示する方法を変更する。
出力の最高値2つではなく、出力値を重みとしたランダムな抽出で選択肢を提示する方法をとった。

\subsubsection{この手法の手順} \label{logic_random}

\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、ディレクションと最終到達階層を記録し、それらを基にスコアを算出する。
\item 2.を10回行い、データを溜める(最大で2000データ、10プレイ*最大20階*10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ゲーム中におけるディレクターの構造}
入力値7個、出力値17個の、Chainerを利用したニューラルネットワーク。
入力値は、プレイヤーのステータス(\pageref{tableBattleStatus}ページ,表\ref{tableBattleStatus}を参照)の値6つに、現在のプレイヤー位置の値を合わせた実数値7個。
出力値は、出現する全ての敵11種類と、出現する全アイテム6種類、それぞれの価値を決める実数値17個。
それらのステータスは先の手法に同じ。表\ref{tableGameItems}(\pageref{tableGameItems}ページ)を参照。
出力値を重みとし、重み付きランダムで出現する敵またはアイテムを2つ選ぶ。

\subsubsection{学習におけるディレクターの構造}
この手法の手順(\ref{logic_random})で示した通り記録された、ディレクターがゲーム中で使用した入出力の値のうち、
選ばれた選択肢(重み付きランダムで選ばれていた2つ)を、算出されたスコア値に変更し、誤差修正関数にかけることで学習を行う。

\begin{table}[hbt]
\caption{重み付きランダムを採用した方法における平均到達階層とスコア}
\label{figure2}
\begin{center}
\includegraphics[scale=0.7]{plot_random_v7.eps}
\end{center}
\end{table}

\subsubsection{結果}
3000回目前後の学習以降、スコアが安定して-5以上出る結果となった。
選択肢を直接決めるのではなく、何が出やすいのか傾向を決めさせることで、選択毎に変化が生じるゲームとなった。
またεによらない通常の学習でもランダム性を持たせることで、現状よりも良い選択を発見する助けにもなっているのでは
ないかと考察できる。
しかし、学習3000回目以降も継続して学習を続けても-5より高いスコアを安定して出すことは叶わなかった。この原因は
学習のし過ぎで高いスコアの状態で難易度が安定していないものと仮定された。

\newpage

\subsection{学習に応じて学習率を低下させていく方法}
先の手法を改良し、直近のスコアから学習の進み具合を見て、学習率を低下させる方法をとった。
先の図において学習3000回目以降、目標付近で揺れ動いているaverage(橙の線)を目標に収束させることが狙いである。

\subsubsection{この手法の手順} 
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、AIディレクターの出力(以下、ディレクションとする)と、最終到達階層を記録し、それらを基にスコアを算出する。
\item 2.を10回行い、データを溜める(最大で2000データ、10プレイ*最大20階*10回)。
\item 3.のスコアに応じて、学習率lrを減少させる。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ディレクターの構造}
この手法は、\ref{random-method}(\pageref{random-method}ページ)の重み付きランダムを採用した方法を改良したものであり、
ディレクターの仕組みは、学習率の変更が為される箇所を除き\ref{random-method}に同じである。

\subsubsection{学習率の変更}
スコアの取得10回毎に行われる学習の直前に、その10回のスコアの平均$avg$に応じて、学習率$lr$を以下のように変更する。
\begin{math}
avg>-5のとき
lr=0.01/(10^(\frac{5+avg}{2}))
avg<=-5のとき
lr=0.01
\end{math}

\begin{table}[hbt]
\caption{学習に応じて学習率を低下させていく方法における平均到達階層とスコア}
\label{figure3}
\begin{center}
\includegraphics[scale=0.7]{plot_random_v10.eps}
\end{center}
\end{table}

\subsubsection{結果}
結果、狙い通りにはならず、スコア-5以上のスコアを安定して出すことはできなかった。
しかし、高いスコアが先の手法よりも早く出るようになった。
-5以上のスコアが多く出てはいるが、学習が不足しているのか、しばしば低いスコアが出るという不安定なものとなった。

\subsection{出現する敵を動的に生成する方法}
-5より高いスコアが出ない原因として、一段階易しい選択肢を提示したときに易しすぎ、逆に一段階難しい選択肢を提示したときに難しすぎる、という「帯に短し襷に長し」状態になっているがために振れ幅が大きくなっているという仮説を立てた。
これまでの手法では、あらかじめ静的に決められた強さを持つ敵を十数種類から選んでいた。
これを動的なものに変更する。

\subsubsection{この手法の手順} \label{logic_enemygen}
\begin{enumerate}
\item 自動でゲームを10回遊ぶ(ルールベースAIによる)。
\item 1.において、ディレクションと最終到達階層を記録し、それらを基にスコアを算出する。
\item 2.を10回行い、データを溜める(最大で1000データ、10プレイ*最大10階*10回)。
\item 3.を教師データとしてディレクターを学習させる。
\item 学習が終ったデータを削除し、1.に戻る。
\end{enumerate}

\subsubsection{ディレクターの仕組み}
敵とアイテムの2択をディレクションとして出力する。

敵を生成するために、ニューラルネットワークを2つ使用する。
片方は、出現する3種類の敵から決める(実数値3つを出力し、そのうち最大のもの)。
もう片方は、敵の能力の倍率を決める実数値1つを出力する。
(ここにイメージ図)

更にもう1つのニューラルネットワークを使用し、出現するアイテムを決める(実数値6つを出力し、そのうち最大のもの)。

この手法で登場する敵およびアイテムは、表\ref{tableGameItems2}(\pageref{tableGameItems2}ページ)の通り。
\begin{table}[hbt]
\caption{ゲーム中に出現する敵およびアイテムのリスト}
\label{tableGameItems2}
\begin{center}
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{4}{|c|}{敵 (xは第2ニューラルネットワークの出力値)} \\ \hline
名称 & 生命力 & 攻撃力 & 防御力 \\ \hline
\hline
スライム & 4*x & 1*x & 2*x \\ \hline
ゴブリン & 3*x & 2*x & 1*x \\ \hline
ドラゴン & 4*x & 2*x & 2*x \\ \hline \hline \hline
\multicolumn{4}{|c|}{アイテム}\\ \hline
名称 & \multicolumn{3}{c|}{効果} \\ \hline
\hline
新しい鎧 & \multicolumn{3}{l|}{最大生命力を5増加} \\ \hline
新しい剣 & \multicolumn{3}{l|}{攻撃力を2増加} \\ \hline
新しい盾 & \multicolumn{3}{l|}{防御力を2増加} \\ \hline
爆弾 & \multicolumn{3}{l|}{爆弾所持数を1増加} \\ \hline
薬草 & \multicolumn{3}{l|}{薬草所持数を1増加} \\ \hline
宿屋 & \multicolumn{3}{l|}{生命力を10回復} \\ \hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{結果}
スコアが-3前後となり、これまでの手法よりも高いスコアとなった。
しかし、これ以上続けてもスコアの向上は認められなかった。
また、この手法では学習1回相当のスコア取得に要するプレイ時間が長くなってしまい、規模の大きいゲームの学習には向かないものと思われる。
\begin{table}[hbt]
\caption{出現する敵を動的に生成する方法における平均到達階層とスコア}
\label{figure3}
\begin{center}
\includegraphics[scale=0.7]{plot_multiply.eps}
\end{center}
\end{table}

\subsection{付録}
\subsubsection{参考文献}
ほげ
\subsubsection{ソースコード}
当研究で使用したゲームおよび学習システムのソースコードは、GitHub上で公開されている。
https://github.com/KaichiHamaishi/aiDirectedRPG

\end{document}
